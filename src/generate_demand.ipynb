{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_demand(od_data):\n",
    "    \"\"\"\n",
    "    randomly generate agent with weighted sampling\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate the total demand from the original dataset\n",
    "    total_demand = od_data['Ton'].sum()\n",
    "\n",
    "    # Step 2: Create dictionaries with the total demand from each origin and destination\n",
    "    origin_demand = od_data.groupby('O')['Ton'].sum().to_dict()\n",
    "    destination_demand = od_data.groupby('D')['Ton'].sum().to_dict()\n",
    "\n",
    "    # Normalize the demands to get probabilities\n",
    "    total_origin_demand = sum(origin_demand.values())\n",
    "    total_destination_demand = sum(destination_demand.values())\n",
    "\n",
    "    assert total_origin_demand == total_destination_demand\n",
    "\n",
    "    origin_probabilities = {o: d / total_origin_demand for o, d in origin_demand.items()}\n",
    "    destination_probabilities = {d: dmd / total_destination_demand for d, dmd in destination_demand.items()}\n",
    "\n",
    "    # Step 3: Generate a new dataset\n",
    "    new_data = []\n",
    "\n",
    "    for _ in range(int(total_demand)):\n",
    "        # Select origin based on weighted probability\n",
    "        origin = np.random.choice(list(origin_probabilities.keys()), p=list(origin_probabilities.values()))\n",
    "        \n",
    "        # Select destination - 70% chance to be either '9', '10', '7', or '18'\n",
    "        if random.random() < 0.7:\n",
    "            destination = random.choice(['9', '10', '7', '18'])\n",
    "        else:\n",
    "            destination = np.random.choice(list(destination_probabilities.keys()), p=list(destination_probabilities.values()))\n",
    "        \n",
    "        # Assuming each entry in the new dataset has a demand of 1 to match the total demand\n",
    "        new_data.append([origin, destination, 1])\n",
    "\n",
    "    # Convert the new data into a DataFrame\n",
    "    new_od_data = pd.DataFrame(new_data, columns=['O', 'D', 'Ton'])\n",
    "\n",
    "    # Aggregate the demand for the same origin to destination pairs\n",
    "    aggregated_od_data = new_od_data.groupby(['O', 'D']).count().reset_index()\n",
    "    return aggregated_od_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demand(od_data):\n",
    "    # Step 1: Calculate the total demand from the original dataset\n",
    "    total_demand = od_data['Ton'].sum()\n",
    "\n",
    "    # Step 2: Create a list of all origin-destination pairs with their corresponding demand\n",
    "    od_demand = od_data.groupby(['O', 'D'])['Ton'].sum().reset_index()\n",
    "\n",
    "    # Normalize the demands to get probabilities\n",
    "    od_demand['Probability'] = od_demand['Ton'] / total_demand\n",
    "\n",
    "    # Create a cumulative probability distribution for efficient sampling\n",
    "    od_demand['Cumulative_Prob'] = od_demand['Probability'].cumsum()\n",
    "\n",
    "    # Step 3: Generate a new dataset\n",
    "    new_data = []\n",
    "\n",
    "    for _ in range(int(total_demand)):\n",
    "        # Randomly select an origin-destination pair based on weighted probability\n",
    "        random_prob = random.random()\n",
    "        selected_pair = od_demand[od_demand['Cumulative_Prob'] >= random_prob].iloc[0]\n",
    "        origin, destination = selected_pair['O'], selected_pair['D']\n",
    "\n",
    "        # 70% chance to override the destination to be either '9', '10', '7', or '18'\n",
    "        if random.random() < 0.7:\n",
    "            destination = random.choice(['9', '10', '7', '18'])\n",
    "\n",
    "        # Assuming each entry in the new dataset has a demand of 1 to match the total demand\n",
    "        new_data.append([origin, destination, 1])\n",
    "\n",
    "    # Convert the new data into a DataFrame\n",
    "    new_od_data = pd.DataFrame(new_data, columns=['O', 'D', 'Ton'])\n",
    "\n",
    "    # Aggregate the demand for the same origin to destination pairs\n",
    "    aggregated_od_data = new_od_data.groupby(['O', 'D']).count().reset_index()\n",
    "    aggregated_od_data.rename(columns={'Ton': 'Total_Demand'}, inplace=True)\n",
    "\n",
    "    return aggregated_od_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the uploaded OD demand file\n",
    "od_data = pd.read_csv(\"../data/SiouxFalls/SiouxFalls_od.csv\")\n",
    "\n",
    "od_df = generate_demand(od_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(od_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_df.to_csv(\"../data/SiouxFalls/SiouxFalls_od_dist2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rollinghorizon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
